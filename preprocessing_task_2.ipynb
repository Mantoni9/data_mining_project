{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d118b338-465b-4211-bd82-ae2c2d114f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 14:47:49.143722: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import cos, sin, arcsin, sqrt\n",
    "from math import radians\n",
    "from datetime import date\n",
    "import holidays\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be652b-7fce-4c0d-b04c-cfce727277ae",
   "metadata": {},
   "source": [
    "## Pre processing for task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adfc4fce-a469-45fd-87a9-7fdaf49c55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with files for stations 2022\n",
    "def prepare_stations():\n",
    "    lst = []\n",
    "    for year in range(2014, 2023):\n",
    "        df = pd.read_csv(f'data/stations/Stations_{year}.csv')\n",
    "        lst.append(df)\n",
    "    \n",
    "    # Remove Duplicates\n",
    "    df_stations = pd.concat(lst, ignore_index=True)\n",
    "    df_stations.drop_duplicates(subset=['code'], inplace=True, keep=\"first\")\n",
    "    df_stations.to_csv(\"data/task_2/all_stations.csv\")\n",
    "    \n",
    "    def distanceToCenter(row):\n",
    "        lon1 = -73.554167\n",
    "        lat1 = 45.508888\n",
    "        lon2 = row['longitude']\n",
    "        lat2 = row['latitude']\n",
    "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "        dlon = lon2 - lon1 \n",
    "        dlat = lat2 - lat1 \n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * arcsin(sqrt(a)) \n",
    "        km = 6367 * c\n",
    "        return km\n",
    "    \n",
    "    df_stations_2022 = pd.read_csv('data/stations/Stations_2022.csv')\n",
    "    df_stations['distance_to_center'] = df_stations_2022.apply(lambda row: distanceToCenter(row), axis=1)\n",
    "    coordinates = df_stations[['latitude', 'longitude']]\n",
    "    kmeans = KMeans(n_clusters=50, random_state=0).fit(coordinates)\n",
    "    df_stations['stations_cluster'] =  kmeans.labels_\n",
    "    \n",
    "    df_stations.to_csv(\"data/task_2/all_stations_clustered.csv\")\n",
    "    return df_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bdcc6606-2470-4380-9da4-01e8babe22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidaysCanada = holidays.country_holidays('CA', subdiv='QC')\n",
    "\n",
    "def all_rides(start_year, end_year):\n",
    "    all_rides = pd.DataFrame()\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(4, 11):\n",
    "            rides_file_path = f'data/bike_rides/OD_{year}-{month:02d}.csv'\n",
    "            df = pd.read_csv(rides_file_path, parse_dates=[\"start_date\", \"end_date\"])\n",
    "            all_rides = pd.concat([all_rides, df])\n",
    "    all_rides['year'] = all_rides['start_date'].dt.year\n",
    "    all_rides['month'] = all_rides['start_date'].dt.month\n",
    "    all_rides['weekday'] = all_rides['start_date'].dt.weekday\n",
    "    all_rides['is_holiday'] = all_rides['start_date'].apply(lambda x: x.date() in holidaysCanada)\n",
    "    all_rides['is_weekend'] = all_rides['start_date'].dt.weekday > 4\n",
    "    all_rides['start_date'] = pd.to_datetime(all_rides['start_date']).dt.date\n",
    "    return all_rides\n",
    "\n",
    "def get_weather():\n",
    "    df_weather = pd.read_csv('data/preprocessed_data/weather.csv', parse_dates=[4])\n",
    "    df_weather.columns = df_weather.columns.str.lower()\n",
    "    df_weather = df_weather[[\"date/time\", \"mean temp (°c)\", \"total precip (mm)\"]]\n",
    "    df_weather = df_weather.rename(columns={\"date/time\": \"tmp_date\",'mean temp (°c)': 'mean_temperature','total precip (mm)': 'total_precipitation'})\n",
    "    df_weather['tmp_date'] = pd.to_datetime(df_weather['tmp_date']).dt.date\n",
    "    \n",
    "    # interpolate missing data\n",
    "    df_weather[['mean_temperature','total_precipitation']] = df_weather[['mean_temperature','total_precipitation']].interpolate()\n",
    "    return df_weather\n",
    "\n",
    "def include_clusters(rides_df, stations_df):\n",
    "    merged_df = pd.merge(rides_df, stations_df, left_on='start_station_code', right_on='code', how='left')\n",
    "    merged_df = merged_df.rename(columns={'stations_cluster': 'start_station_cluster'})\n",
    "    merged_df.drop('code', axis=1, inplace=True)\n",
    "    \n",
    "    merged_df = pd.merge(merged_df, stations_df, left_on='end_station_code', right_on='code', how='left')\n",
    "    merged_df = merged_df.rename(columns={'stations_cluster': 'end_station_cluster'})\n",
    "    merged_df.drop('code', axis=1, inplace=True)\n",
    "    \n",
    "    grouped_df = merged_df.groupby(['start_date', 'start_station_cluster', 'end_station_cluster']).agg(count=('start_date', 'size'), duration_sec=('duration_sec', 'mean'), is_holiday=(\"is_holiday\", \"first\"), is_weekend=(\"is_weekend\", \"first\")).reset_index()\n",
    "    return grouped_df\n",
    "    \n",
    "def include_weather(grouped, weather):\n",
    "    w_weather = grouped.merge(weather, left_on='start_date', right_on='tmp_date', how='left')\n",
    "    w_weather.drop('tmp_date', axis=1, inplace=True)\n",
    "    return w_weather\n",
    "\n",
    "# run this function to complete the pre-processing for task 2\n",
    "def complete_pre_task2():\n",
    "    # Training\n",
    "    rides = all_rides(2014, 2018)\n",
    "    stations = prepare_stations()[[\"code\", \"stations_cluster\"]]\n",
    "    grouped = include_clusters(rides, stations)\n",
    "    weather = get_weather()\n",
    "    include_weather(grouped, weather).to_csv(\"data/task_2/pre_task2_2014_2018.csv\")\n",
    "    \n",
    "    # Validation\n",
    "    rides = all_rides(2019, 2019)\n",
    "    stations = prepare_stations()[[\"code\", \"stations_cluster\"]]\n",
    "    grouped = include_clusters(rides, stations)\n",
    "    weather = get_weather()\n",
    "    include_weather(grouped, weather).to_csv(\"data/task_2/pre_task2_2019.csv\")\n",
    "    \n",
    "    # Test\n",
    "    rides = all_rides(2022, 2022)\n",
    "    rides = rides.rename(columns={'emplacement_pk_start': 'start_station_code',\n",
    "                                        'emplacement_pk_end': 'end_station_code'})\n",
    "    stations = prepare_stations()[[\"code\", \"stations_cluster\"]]\n",
    "    grouped = include_clusters(rides, stations)\n",
    "    weather = get_weather()\n",
    "    include_weather(grouped, weather).to_csv(\"data/task_2/pre_task2_2022.csv\")\n",
    "    \n",
    "    \n",
    "# function for scaling values\n",
    "def model_prep(train, valid, test):\n",
    "    df_train = train.copy()\n",
    "    df_valid = valid.copy()\n",
    "    df_test = test.copy()\n",
    "    \n",
    "    # One hot encoding of Boolean variables\n",
    "    encoder = OneHotEncoder()\n",
    "    encoded = pd.DataFrame(encoder.fit_transform(df_train[['is_holiday', 'is_weekend']]).toarray(), columns=encoder.get_feature_names_out(['is_holiday', 'is_weekend']))\n",
    "    df_train = df_train.drop(columns=['is_holiday', 'is_weekend'])\n",
    "    df_train = df_train.join(encoded)\n",
    "    encoded = pd.DataFrame(encoder.transform(df_valid[['is_holiday', 'is_weekend']]).toarray(), columns=encoder.get_feature_names_out(['is_holiday', 'is_weekend']))\n",
    "    df_valid = df_valid.drop(columns=['is_holiday', 'is_weekend'])\n",
    "    df_valid = df_valid.join(encoded)\n",
    "    encoded = pd.DataFrame(encoder.transform(df_test[['is_holiday', 'is_weekend']]).toarray(), columns=encoder.get_feature_names_out(['is_holiday', 'is_weekend']))\n",
    "    df_test = df_test.drop(columns=['is_holiday', 'is_weekend'])\n",
    "    df_test = df_test.join(encoded)\n",
    "    \n",
    "    # Standard scaler for continuous variables\n",
    "    scaler = StandardScaler()\n",
    "    df_train[['duration_sec', 'mean_temperature', 'total_precipitation']] = scaler.fit_transform(df_train[['duration_sec', 'mean_temperature', 'total_precipitation']])\n",
    "    df_valid[['duration_sec', 'mean_temperature', 'total_precipitation']] = scaler.transform(df_valid[['duration_sec', 'mean_temperature', 'total_precipitation']])\n",
    "    df_test[['duration_sec', 'mean_temperature', 'total_precipitation']] = scaler.transform(df_test[['duration_sec', 'mean_temperature', 'total_precipitation']])\n",
    "    \n",
    "    # Minmax scaler for station cluster ids\n",
    "    scaler = MinMaxScaler()\n",
    "    df_train[['start_station_cluster', 'end_station_cluster']] = scaler.fit_transform(df_train[['start_station_cluster', 'end_station_cluster']])\n",
    "    df_valid[['start_station_cluster', 'end_station_cluster']] = scaler.transform(df_valid[['start_station_cluster', 'end_station_cluster']])\n",
    "    df_test[['start_station_cluster', 'end_station_cluster']] = scaler.transform(df_test[['start_station_cluster', 'end_station_cluster']])\n",
    "    \n",
    "    return df_train, df_valid, df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a3e2e3-e580-4d7e-9a8e-692418bacde5",
   "metadata": {},
   "source": [
    "## Modelling task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d5e3803-6072-4505-804e-5147f46af6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/task_2/pre_task2_2014_2018.csv\", index_col=0)\n",
    "date_train = df_train['start_date']\n",
    "df_train = df_train.drop([\"start_date\"], axis=1)\n",
    "\n",
    "df_valid = pd.read_csv(\"data/task_2/pre_task2_2019.csv\", index_col=0)\n",
    "date_valid = df_valid['start_date']\n",
    "df_valid = df_valid.drop([\"start_date\"], axis=1)\n",
    "\n",
    "df_test = pd.read_csv(\"data/task_2/pre_task2_2022.csv\", index_col=0)\n",
    "date_test = df_test['start_date']\n",
    "df_test = df_test.drop([\"start_date\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1afb732f-647e-41a3-99e7-a2bbf00803ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def df_split(train, valid, test):\n",
    "    X_train = train.copy()\n",
    "    y_train = X_train['count']\n",
    "    X_train = X_train.drop([\"count\", 'duration_sec'], axis=1)\n",
    "    \n",
    "    X_valid = valid.copy()\n",
    "    y_valid = X_valid['count']\n",
    "    X_valid = X_valid.drop([\"count\", 'duration_sec'], axis=1)\n",
    "    \n",
    "    X_test = test.copy()\n",
    "    y_test = X_test['count']\n",
    "    X_test = X_test.drop([\"count\", 'duration_sec'], axis=1)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "def random_forrest_regressor(train, valid, test):\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = df_split(train, valid, test)\n",
    "    \n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    res = X_test.copy()\n",
    "    res[\"actual\"] = y_test\n",
    "    res[\"pred\"] = y_pred\n",
    "\n",
    "    res.to_csv(\"results/pred_random_forrest_regressor_all.csv\")\n",
    "\n",
    "def gradient_boosting_regression(train, valid, test):\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = df_split(train, valid, test)\n",
    "   \n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    res = X_test.copy()\n",
    "    res[\"actual\"] = y_test\n",
    "    res[\"pred\"] = y_pred\n",
    "    res.to_csv(\"results/pred_gradient_boosting_regressor_all.csv\")\n",
    "\n",
    "def tensor_flow(train, valid, test):\n",
    "    train, valid, test = model_prep(train, valid, test)\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = df_split(train, valid, test)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_valid, y_valid))\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    res = X_test.copy()\n",
    "    res[\"actual\"] = y_test\n",
    "    res[\"pred\"] = y_pred\n",
    "    res.to_csv(\"results/pred_tensorflow_all.csv\")\n",
    "\n",
    "def cat_boost(train, valid, test):\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = df_split(train, valid, test)\n",
    "    \n",
    "    model = CatBoostRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(y_pred)\n",
    "    \n",
    "    res = X_test.copy()\n",
    "    res[\"actual\"] = y_test\n",
    "    res[\"pred\"] = y_pred\n",
    "    res.to_csv(\"results/pred_cat_boost_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b37cd4",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755db426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner\n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('dense_1_units', min_value=16, max_value=128, step=16), \n",
    "        input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    for n_nodes in range(hp.Int('nnodes', min_value=0, max_value=5, step=1)):\n",
    "        model.add(Dense(\n",
    "            units=hp.Int('dense_{0}_units'.format(n_nodes+2), min_value=16, max_value=128, step=16), \n",
    "            activation='relu'))   \n",
    "    model.add(Dense(1))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Int('lrx10', min_value=1, max_value=5, step=1)/10)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def hp_tensorflow(train, valid, test):\n",
    "    train, valid, test = model_prep(train, valid, test)\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = df_split(train, valid, test)\n",
    "    \n",
    "    tuner_search = keras_tuner.RandomSearch(build_model,\n",
    "                                       objective='val_loss',\n",
    "                                       max_trials=20,\n",
    "                                       directory = \".\",\n",
    "                                       project_name=\"nn_param_tune\",\n",
    "                                       overwrite=True)\n",
    "\n",
    "    tuner_search.search(X_train, y_train, epochs=10, \n",
    "                        validation_data=(X_valid, y_valid))\n",
    "    \n",
    "    tuner_search.results_summary(1)\n",
    "    \n",
    "    model = tuner_search.get_best_models(num_models=1)[0]\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=64, \n",
    "                        validation_data=(X_valid, y_valid))\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    res = X_test.copy()\n",
    "    res[\"actual\"] = y_test\n",
    "    res[\"pred\"] = y_pred\n",
    "    res.to_csv(\"results/pred_tensorflow_all_hp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da09f2-4d1d-4c52-8795-ee24f4819ec7",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8f9bd6f-daf3-4df8-b3cc-a8cf43408e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "rfr_res = \"results/pred_random_forrest_regressor_all.csv\"\n",
    "gbr_res = \"results/pred_gradient_boosting_regressor_all.csv\"\n",
    "tfl_res = \"results/pred_tensorflow_all.csv\"\n",
    "cat_res = \"results/pred_cat_boost_all.csv\"\n",
    "\n",
    "def evaluate(filepath):\n",
    "    res = pd.read_csv(filepath)\n",
    "    actual_values = res['actual']\n",
    "    predicted_values = res['pred']\n",
    "    mse = mean_squared_error(actual_values, predicted_values)\n",
    "    rmse = np.sqrt(mse)\n",
    "    correlation_coefficient, p_value = pearsonr(actual_values, predicted_values)\n",
    "    return rmse, mse, correlation_coefficient, p_value\n",
    "\n",
    "def compare_results(all_result_files):    \n",
    "    results = []\n",
    "    for filepath in all_result_files:\n",
    "        rmse, mse, correlation_coefficient, p_value = evaluate(filepath)\n",
    "        result = {\n",
    "            'Model': filepath[12:].split(\".\")[0].replace(\"_\", \" \").replace(\" all\", \"\"),  # Extract the model name from the filepath\n",
    "            'RMSE': rmse,\n",
    "            'MSE': mse,\n",
    "            'Correlation Coefficient': correlation_coefficient,\n",
    "            'P-Value': p_value\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"results/evaluations.csv\")\n",
    "    print(results_df)\n",
    "\n",
    "all_result_files = [rfr_res, gbr_res, tfl_res, cat_res]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
