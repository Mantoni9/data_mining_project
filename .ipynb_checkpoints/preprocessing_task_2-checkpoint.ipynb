{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d118b338-465b-4211-bd82-ae2c2d114f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import cos, sin, arcsin, sqrt\n",
    "from math import radians\n",
    "from datetime import date\n",
    "import holidays\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be652b-7fce-4c0d-b04c-cfce727277ae",
   "metadata": {},
   "source": [
    "## Pre processing for task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfc4fce-a469-45fd-87a9-7fdaf49c55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with files for stations 2022\n",
    "def prepare_stations():\n",
    "    lst = []\n",
    "    for year in range(2014, 2023):\n",
    "        df = pd.read_csv(f'data/stations/Stations_{year}.csv')\n",
    "        lst.append(df)\n",
    "    \n",
    "    # Remove Duplicates\n",
    "    df_stations = pd.concat(lst, ignore_index=True)\n",
    "    df_stations.drop_duplicates(subset=['code'], inplace=True, keep=\"first\")\n",
    "    df_stations.to_csv(\"data/task_2/all_stations.csv\")\n",
    "    \n",
    "    def distanceToCenter(row):\n",
    "        lon1 = -73.554167\n",
    "        lat1 = 45.508888\n",
    "        lon2 = row['longitude']\n",
    "        lat2 = row['latitude']\n",
    "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "        dlon = lon2 - lon1 \n",
    "        dlat = lat2 - lat1 \n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * arcsin(sqrt(a)) \n",
    "        km = 6367 * c\n",
    "        return km\n",
    "    \n",
    "    df_stations['distance_to_center'] = df_stations_2022.apply(lambda row: distanceToCenter(row), axis=1)\n",
    "    coordinates = df_stations[['latitude', 'longitude']]\n",
    "    kmeans = KMeans(n_clusters=50, random_state=0).fit(coordinates)\n",
    "    df_stations['stations_cluster'] =  kmeans.labels_\n",
    "    \n",
    "    df_stations.to_csv(\"data/task_2/all_stations_clustered.csv\")\n",
    "    return df_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdcc6606-2470-4380-9da4-01e8babe22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidaysCanada = holidays.country_holidays('CA', subdiv='QC')\n",
    "\n",
    "def all_rides(start_year, end_year):\n",
    "    all_rides = pd.DataFrame()\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(4, 11):\n",
    "            rides_file_path = f'data/bike_rides/OD_{year}-{month:02d}.csv'\n",
    "            df = pd.read_csv(rides_file_path, parse_dates=[\"start_date\", \"end_date\"])\n",
    "            all_rides = pd.concat([all_rides, df])\n",
    "    all_rides['year'] = all_rides['start_date'].dt.year\n",
    "    all_rides['month'] = all_rides['start_date'].dt.month\n",
    "    all_rides['weekday'] = all_rides['start_date'].dt.weekday\n",
    "    all_rides['is_holiday'] = all_rides['start_date'].apply(lambda x: x.date() in holidaysCanada)\n",
    "    all_rides['is_weekend'] = all_rides['start_date'].dt.weekday > 4\n",
    "    all_rides['start_date'] = pd.to_datetime(all_rides['start_date']).dt.date\n",
    "    return all_rides\n",
    "\n",
    "def get_weather():\n",
    "    df_weather = pd.read_csv('data/preprocessed_data/weather.csv', parse_dates=[4])\n",
    "    df_weather.columns = df_weather.columns.str.lower()\n",
    "    df_weather = df_weather[[\"date/time\", \"mean temp (°c)\", \"total precip (mm)\"]]\n",
    "    df_weather = df_weather.rename(columns={\"date/time\": \"tmp_date\",'mean temp (°c)': 'mean_temperature','total precip (mm)': 'total_precipitation'})\n",
    "    df_weather['tmp_date'] = pd.to_datetime(df_weather['tmp_date']).dt.date\n",
    "    \n",
    "    # interpolate missing data\n",
    "    df_weather[['mean_temperature','total_precipitation']] = df_weather[['mean_temperature','total_precipitation']].interpolate()\n",
    "    return df_weather\n",
    "\n",
    "def include_clusters(rides_df, stations_df):\n",
    "    merged_df = pd.merge(rides_df, stations_df, left_on='start_station_code', right_on='code', how='left')\n",
    "    merged_df = merged_df.rename(columns={'stations_cluster': 'start_station_cluster'})\n",
    "    merged_df.drop('code', axis=1, inplace=True)\n",
    "    \n",
    "    merged_df = pd.merge(merged_df, stations_df, left_on='end_station_code', right_on='code', how='left')\n",
    "    merged_df = merged_df.rename(columns={'stations_cluster': 'end_station_cluster'})\n",
    "    merged_df.drop('code', axis=1, inplace=True)\n",
    "    \n",
    "    grouped_df = merged_df.groupby(['start_date', 'start_station_cluster', 'end_station_cluster']).agg(count=('start_date', 'size'), duration_sec=('duration_sec', 'mean'), is_holiday=(\"is_holiday\", \"first\"), is_weekend=(\"is_weekend\", \"first\")).reset_index()\n",
    "    return grouped_df\n",
    "    \n",
    "def include_weather(grouped, weather):\n",
    "    w_weather = grouped.merge(weather, left_on='start_date', right_on='tmp_date', how='left')\n",
    "    w_weather.drop('tmp_date', axis=1, inplace=True)\n",
    "    return w_weather\n",
    "\n",
    "# run this function to complete the pre-processing for task 2\n",
    "def complete_pre_task2():\n",
    "    rides = all_rides(2014, 2018)\n",
    "    stations = prepare_stations()[[\"code\", \"stations_cluster\"]]\n",
    "    grouped = include_clusters(rides, stations)\n",
    "    weather = get_weather()\n",
    "    include_weather(grouped, weather).to_csv(\"data/task_2/pre_task2_2014_2018.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a3e2e3-e580-4d7e-9a8e-692418bacde5",
   "metadata": {},
   "source": [
    "## Modelling task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d5e3803-6072-4505-804e-5147f46af6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/task_2/pre_task2_2014_2018.csv\", index_col=0)\n",
    "df = df.drop([\"start_date\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1afb732f-647e-41a3-99e7-a2bbf00803ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def random_forrest_regressor(df):\n",
    "    X = df\n",
    "    y = df['count']\n",
    "    X = X.drop(\"count\", axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    res = X_test.copy()\n",
    "    res[\"actual\"] = y_test\n",
    "    res[\"pred\"] = y_pred\n",
    "\n",
    "    res.to_csv(\"results/pred_random_forrest_regressor_all.csv\")\n",
    "\n",
    "def gradient_boosting_regression(df):\n",
    "    X = df\n",
    "    y = df['count']\n",
    "    X = X.drop(\"count\", axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "   \n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    res = X_test.copy()\n",
    "    res[\"actual\"] = y_test\n",
    "    res[\"pred\"] = y_pred\n",
    "    res.to_csv(\"results/pred_gradient_boosting_regressor_all.csv\")\n",
    "\n",
    "def tensor_flow(df):\n",
    "    X = df\n",
    "    y = df['count']\n",
    "    X = X.drop(\"count\", axis=1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(X_scaled.shape[1],)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X_scaled, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "    new_data_scaled = scaler.transform(X_test)\n",
    "    y_pred = model.predict(new_data_scaled)\n",
    "    \n",
    "    res = X_test.copy()\n",
    "    res[\"actual\"] = y_test\n",
    "    res[\"pred\"] = y_pred\n",
    "    res.to_csv(\"results/pred_tensorflow_all.csv\")\n",
    "\n",
    "def cat_boost(df):\n",
    "    X = df\n",
    "    y = df['count']\n",
    "    X = X.drop(\"count\", axis=1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = CatBoostRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(y_pred)\n",
    "    \n",
    "    res = X_test.copy()\n",
    "    res[\"actual\"] = y_test\n",
    "    res[\"pred\"] = y_pred\n",
    "    res.to_csv(\"results/pred_cat_boost_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da09f2-4d1d-4c52-8795-ee24f4819ec7",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d8f9bd6f-daf3-4df8-b3cc-a8cf43408e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "rfr_res = \"results/pred_random_forrest_regressor_all.csv\"\n",
    "gbr_res = \"results/pred_gradient_boosting_regressor_all.csv\"\n",
    "tfl_res = \"results/pred_tensorflow_all.csv\"\n",
    "cat_res = \"results/pred_cat_boost_all.csv\"\n",
    "\n",
    "def evaluate(filepath):\n",
    "    res = pd.read_csv(filepath)\n",
    "    actual_values = res['actual']\n",
    "    predicted_values = res['pred']\n",
    "    mse = mean_squared_error(actual_values, predicted_values)\n",
    "    rmse = np.sqrt(mse)\n",
    "    correlation_coefficient, p_value = pearsonr(actual_values, predicted_values)\n",
    "    return rmse, mse, correlation_coefficient, p_value\n",
    "\n",
    "def compare_results(all_result_files):    \n",
    "    results = []\n",
    "    for filepath in all_result_files:\n",
    "        rmse, mse, correlation_coefficient, p_value = evaluate(filepath)\n",
    "        result = {\n",
    "            'Model': filepath[12:].split(\".\")[0].replace(\"_\", \" \").replace(\" all\", \"\"),  # Extract the model name from the filepath\n",
    "            'RMSE': rmse,\n",
    "            'MSE': mse,\n",
    "            'Correlation Coefficient': correlation_coefficient,\n",
    "            'P-Value': p_value\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"results/evaluations.csv\")\n",
    "    print(results_df)\n",
    "\n",
    "all_result_files = [rfr_res, gbr_res, tfl_res, cat_res]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
